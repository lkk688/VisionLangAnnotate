# vLLM Backend for VisionLangAnnotate

This module provides a vLLM-based backend for the VisionLangAnnotate project. vLLM is a high-performance library for LLM inference and serving, offering significant speedups compared to traditional implementations.

## Features

- Fast text generation using vLLM's optimized implementation
- Support for tensor parallelism across multiple GPUs
- Streaming response capability
- Compatible with the VLMBackend interface

## Installation

To use the vLLM backend, you need to install the vLLM package:

```bash
pip install vllm
```

Note: vLLM requires CUDA and is optimized for NVIDIA GPUs. For detailed installation instructions, see the [vLLM documentation](https://github.com/vllm-project/vllm).

## Usage

### Basic Usage

```python
from PIL import Image
from VisionLangAnnotateModels.VLM.vllm_utils import VLLMBackend
from VisionLangAnnotateModels.VLM.vlm_classifierv3 import VLMClassifier

# Create a vLLM backend
backend = VLLMBackend(model_name="meta-llama/Llama-2-7b-chat-hf")

# Create a classifier with the backend
classifier = VLMClassifier(backend=backend)

# Use the classifier
images = [Image.open("image.jpg")]
prompts = ["Describe this image"]
results = classifier.classify(list(zip(images, prompts)))
```

### Streaming Text Generation

```python
from VisionLangAnnotateModels.VLM.vllm_utils import process_with_vllm_text, text_stream_callback

# Process text with streaming
result = process_with_vllm_text(
    prompt="Explain the concept of deep learning.",
    vllm_model="meta-llama/Llama-2-7b-chat-hf",
    stream=True,
    stream_callback=text_stream_callback
)
```

### Using Tensor Parallelism

```python
from VisionLangAnnotateModels.VLM.vllm_utils import VLLMBackend

# Create a backend with tensor parallelism
backend = VLLMBackend(
    model_name="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=2  # Use 2 GPUs
)
```

## Limitations

- The vLLM backend does not support vision models directly. It's designed for text-only processing.
- When used with the VLMClassifier, it ignores the image inputs and only processes the text prompts.
- This backend is most useful for post-processing text descriptions generated by vision models or for standardizing outputs.

## Testing

You can test the vLLM backend using the provided test script:

```bash
python -m VisionLangAnnotateModels.VLM.test_vllm
```

Or run specific tests:

```bash
python -m VisionLangAnnotateModels.VLM.test_vllm --test backend
python -m VisionLangAnnotateModels.VLM.test_vllm --test streaming
```

## Performance Considerations

- vLLM uses PagedAttention for efficient memory management, allowing it to handle longer sequences and more concurrent requests.
- The `tensor_parallel_size` parameter can be adjusted based on the number of available GPUs to improve performance.
- For optimal performance, use models that are compatible with vLLM's optimizations.

## Supported Models

The vLLM backend supports a wide range of text-only models, including:

- LLaMA and LLaMA 2
- Mistral
- Mixtral
- Falcon
- MPT
- Pythia
- StableLM
- And many others

For a complete list, refer to the [vLLM documentation](https://github.com/vllm-project/vllm).


cmpe@cmpe-4090:/models$ docker run --gpus all -it --rm \
  -v /models:/models \
  -p 8001:8000 \
  --name vllm-debug \
  vllm-container \
  bash

root@bb6c9a5f7756:/models# cd llama.cpp/
root@bb6c9a5f7756:/models/llama.cpp# mkdir build && cd build
root@bb6c9a5f7756:/models/llama.cpp/build# cmake .. -DGGML_CUDA=ON -DCUDAToolkit_ROOT=/usr/local/cuda && \

root@bb6c9a5f7756:/models/llama.cpp/build# sudo apt install libcurl4-openssl-dev

root@bb6c9a5f7756:/models/llama.cpp/build# ls /usr/local/cuda/lib64/libcubla*   
/usr/local/cuda/lib64/libcublas.so.12        /usr/local/cuda/lib64/libcublasLt.so.12
/usr/local/cuda/lib64/libcublas.so.12.8.4.1  /usr/local/cuda/lib64/libcublasLt.so.12.8.4.1
root@bb6c9a5f7756:/models/llama.cpp/build# ln -s /usr/local/cuda/lib64/libcublas.so.12 /usr/local/cuda/lib64/libcublas.so
ln -s /usr/local/cuda/lib64/libcublasLt.so.12 /usr/local/cuda/lib64/libcublasLt.so
root@bb6c9a5f7756:/models/llama.cpp/build# cmake .. -DGGML_CUDA=ON -DCUDAToolkit_ROOT=/usr/local/cuda &&     cmake --build . --config Release

bb6c9a5f7756:/models/llama.cpp/build# find / -name cublas_v2.h 2>/dev/null
/opt/venv/lib/python3.12/site-packages/nvidia/cublas/include/cublas_v2.h
root@bb6c9a5f7756:/models/llama.cpp/build# which python
/opt/venv/bin/python
root@bb6c9a5f7756:/models/llama.cpp/build# python -V
Python 3.12.3
root@bb6c9a5f7756:/models/llama.cpp/build# ln -s /opt/venv/lib/python3.12/site-packages/nvidia/cublas/include/cublas_v2.h /usr/local/cuda/include/cublas_v2.h
root@bb6c9a5f7756:/models/llama.cpp/build# cmake .. -DGGML_CUDA=ON -DCUDAToolkit_ROOT=/usr/local/cuda &&     cmake --build . --config Release

root@bb6c9a5f7756:/models/llama.cpp/build# ln -s /opt/venv/lib/python3.12/site-packages/nvidia/cublas/include/* /usr/local/cuda/include/
ln: failed to create symbolic link '/usr/local/cuda/include/cublas_v2.h': File exists
root@bb6c9a5f7756:/models/llama.cpp/build# cmake .. -DGGML_CUDA=ON -DCUDAToolkit_ROOT=/usr/local/cuda &&     cmake --build . --config Release


# LlamaCpp Utils for Vision Language Models

This module provides utilities for interacting with vision-capable language models using the llama-cpp-python server. It's designed to be similar to the `ollama_utils.py` module but specifically for models running through llama-cpp-python's OpenAI-compatible API server.

## Features

- Text processing with LLMs
- Vision processing with multimodal LLMs
- Support for single and multiple image processing
- Streaming responses
- JSON response formatting
- Batch processing of VLM descriptions for classification

## Requirements

- Python 3.8+
- llama-cpp-python with OpenAI-compatible server running
- PIL (Pillow) for image processing
- requests for API communication

## Installation

1. Install llama-cpp-python:

```bash
pip install llama-cpp-python
```

2. Start the llama-cpp-python server with a vision-capable model:

```bash
python -m llama_cpp.server --model /path/to/llava-1.5-7b-q4.gguf --n_gpu_layers -1
```

## Usage

### Basic Text Processing

```python
from llamacpp_utils import LlamaCppClient

# Initialize the client
client = LlamaCppClient()

# Process text
response = client.process_text(
    prompt="What is the capital of France?",
    model="llama-3-8b-q4",
    system_prompt="You are a helpful assistant."
)

print(response)
```

### Vision Processing (Single Image)

```python
from llamacpp_utils import LlamaCppClient
from PIL import Image

# Initialize the client
client = LlamaCppClient()

# Process a single image
image_path = "path/to/image.jpg"
# Alternatively, you can use a PIL Image object
# image = Image.open("path/to/image.jpg")

response = client.process_vision(
    images=image_path,
    prompt="Describe this image in detail.",
    model="llava-1.5-7b-q4"
)

print(response)
```

### Vision Processing (Multiple Images)

```python
from llamacpp_utils import LlamaCppClient

# Initialize the client
client = LlamaCppClient()

# Process multiple images individually
image_paths = ["path/to/image1.jpg", "path/to/image2.jpg"]

responses = client.process_vision(
    images=image_paths,
    prompt="Describe this image in detail.",
    model="llava-1.5-13b-q4"
)

for i, response in enumerate(responses):
    print(f"Response for image {i+1}:")
    print(response)

# Process multiple images together
combined_response = client.process_vision(
    images=image_paths,
    prompt="Compare these images and describe their differences.",
    model="llava-1.5-13b-q4",
    combined_prompt="Compare these images and describe their differences."
)

print("Combined response:")
print(combined_response)
```

### Streaming Responses

```python
from llamacpp_utils import LlamaCppClient

# Define a callback function for streaming
def print_stream(text):
    print(text, end="")

# Initialize the client with the stream callback
client = LlamaCppClient(stream_callback=print_stream)

# Stream text response
client.process_text(
    prompt="Tell me a short story about a robot.",
    model="llama-3-8b-q4",
    stream=True
)

# Stream vision response
client.process_vision(
    images="path/to/image.jpg",
    prompt="Describe this image in detail.",
    model="llava-1.5-7b-q4",
    stream=True
)
```

### Classification of VLM Descriptions

```python
from llamacpp_utils import process_with_llamacpp

# Define VLM descriptions
vlm_descriptions = [
    {
        "image_id": "img1",
        "object_id": 1,
        "description": "A red apple on a wooden table."
    },
    {
        "image_id": "img1",
        "object_id": 2,
        "description": "A green banana next to the apple."
    },
    {
        "image_id": "img2",
        "object_id": 1,
        "description": "A silver laptop with the screen open."
    }
]

# Define allowed classes
allowed_classes = ["fruit", "vegetable", "electronics", "furniture"]

# Process the descriptions
results = process_with_llamacpp(
    vlm_descriptions=vlm_descriptions,
    allowed_classes=allowed_classes,
    model="llama-3-8b-q4"
)

for result in results:
    print(f"Object: {result['description']}")
    print(f"Class: {result['class']}")
    print(f"Confidence: {result['confidence']}")
    print(f"Reasoning: {result['reasoning']}")
    print()
```

### Utility Functions

The module also provides utility functions for simpler usage:

```python
from llamacpp_utils import process_with_llamacpp_text, process_with_llamacpp_vision

# Process text
text_response = process_with_llamacpp_text(
    prompt="What is the capital of France?",
    model="llama-3-8b-q4"
)

# Process vision
vision_response = process_with_llamacpp_vision(
    images="path/to/image.jpg",
    prompt="Describe this image in detail.",
    model="llava-1.5-7b-q4"
)
```

## API Configuration

By default, the client connects to the following endpoints:

- Completions API: `http://localhost:8000/v1/completions`
- Chat API: `http://localhost:8000/v1/chat/completions`

You can customize these URLs when initializing the client:

```python
from llamacpp_utils import LlamaCppClient

client = LlamaCppClient(
    api_url="http://custom-server:8000/v1/completions",
    chat_api_url="http://custom-server:8000/v1/chat/completions",
    timeout=60  # Custom timeout in seconds
)
```

## Error Handling

The client includes error handling for API requests, with fallback from the chat API to the completions API if needed. It also handles JSON parsing errors and provides default results when necessary.

## Image Processing

Images are automatically processed for vision models:

1. Resized to be multiples of 28 pixels (common requirement for vision models)
2. Converted to RGB format if needed
3. Encoded as base64 for API requests

## Notes on Vision Models

To use vision capabilities, you need to run llama-cpp-python with a vision-capable model like Llava. Make sure to start the server with the appropriate model and configuration.

Example of starting the server with a vision model:

```bash
python -m llama_cpp.server --model /path/to/llava-1.5-7b-q4.gguf --n_gpu_layers -1 --clip_model_path /path/to/clip-model.gguf
```

Refer to the llama-cpp-python documentation for more details on running vision models.